import reimport collectionsimport numpy as npimport pandas as pdfrom typing import List, Set, Dict, Pattern, Callable################################################################################# Unified Medical Language System (UMLS)# ###############################################################################class UMLS(object):    """    Simple interface for loading UMLS Metathesaurus ontologies.    https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus    This requires running a query on a previously setup    UMLS instance to generate a data snapshot    TODO this is still too slow    TODO setup to load directly from REF dumps vs. custom UMLS query dump    """    def __init__(self,                  data_root: str,                 min_size: int = 100,                  stopwords: Set = None,                 min_char_len: int = 2,                 max_tok_len: int = 6,                 min_dict_size: int = 50,                 transforms: Dict[str, List[Callable]] = None,                 verbose: bool = False,                 load_cuis: bool = False) -> None:        """        TODO - better way of handling CUI loading        """        self.verbose         = verbose        self.min_size        = min_size         self.stopwords       = stopwords if stopwords else {}        self.load_cuis       = load_cuis        self.min_dict_size   = min_dict_size        self.min_char_len    = min_char_len        self.max_tok_len     = max_tok_len        self.transforms      = transforms if transforms else {}                # filter non-English & zoonotic ontologies        non_en_langs = [            'BAQ', 'CHI', 'CZE', 'DAN', 'DUT', 'ENG', 'EST', 'FIN', 'FRE',            'GER', 'GRE', 'HEB', 'HUN', 'ITA', 'JPN', 'KOR', 'LAV', 'NOR',            'POL', 'POR', 'RUS', 'SCR', 'SPA', 'SWE', 'TUR'        ]        zoonotic = ['SNOMEDCT_VET']        langs = '|'.join(non_en_langs + zoonotic)        rgx = r'''[-][A-Z]{2}$|(''' + langs + ''')$'''        self._load(data_root, filter_rgx=rgx)    def get_term_stys(self, term, concepts=None):        stys = set()        for key in self.dictionary:            sab,sty = key            if concepts and sty not in concepts:                continue            if term in self.dictionary[key]:                stys.add(key)        return sorted(list(stys))    def _apply_transforms(self, term:str, sty:str) -> str:        """Apply Source Vocabulary (SAB)-specific transformations        Parameters        ----------        sty        term        Returns        -------        """        # Universal transforms (apply to all terms)        if '*' in self.transforms:            for tf in self.transforms['*']:                term = tf(term.strip())                # STY specific transforms        if sty in self.transforms:            for tf in self.transforms[sty]:                term = tf(term.strip())                return None if not term else term    def _load(self,              fpath: str,              filter_rgx:Pattern = None):        """Load UMLS from parquet file        Parameters        ----------        fpath        filter_rgx        Returns        -------        """        def include(t):            return t and len(t) >= self.min_char_len and \                   t.count(' ') <= self.max_tok_len and \                   t not in self.stopwords and \                   not re.search(r'''^[0-9]+$''', t)        self.dictionary = {}        for (sab, sty), rows in pd.read_parquet(fpath).groupby(['SAB','STY']):            if filter_rgx and re.search(filter_rgx, sab):                continue            if len(rows.STR) < self.min_dict_size:                continue            sty = sty.lower().replace(' ', '_')            terms = {self._apply_transforms(t, sty) \                         if self.transforms else t for t in rows.STR if t}            self.dictionary[(sab, sty)] = {t for t in terms if include(t)}################################################################################# Tools################################################################################def comma_flip(s):    if s.count(',') > 1 or s.count(',') == 0:        return s    if re.search('[,] (with(out)*|and|in) ', s, re.I):        return s    v = map(lambda x: x.strip(), s.split(",")[::-1])    return "{} {}".format(*v).strip()def load_sem_groups(fpath, groupby='GUI', normalize=True):    """    Load Semantic Groups mappings    """    d = collections.defaultdict(list)    df = pd.read_csv(        fpath,        sep="|", header=None,        names=['GUI', 'GROUP', 'TUI', 'STY']    )    for i, row in df.iterrows():        sty = row['STY'] \            if not normalize else row['STY'].replace(" ", "_").lower()        d[row[groupby] if groupby in row else groupby].append(sty)    return dict(d)def get_umls_coverage(dataset, umls, concepts, ignore=[], custom={}):    """    """    # normalize names    concepts = {name.replace(' ', '_').lower() for name in concepts}    oov = collections.Counter()    candidates = collections.defaultdict(int)    for i, c in enumerate(dataset):        # skip discontinuous entities        if len(c.span) > 1:            continue        matched = False        for sab, sty in umls.dictionary:            if sty not in concepts or sty in ignore:                continue            if matched:                break            d = umls.dictionary[(sab, sty)]            # common term variations            term = re.sub(r'''\s''', ' ', c.text)            term_cands = [                term,                term.lower(),                term.rstrip('s').lower()  # remove plural            ]            for t in term_cands:                if t in d or t in custom:                    candidates[c.text.lower()] += 1                    matched = True                    break                if matched:                    break        if not matched:            oov[c.text] += 1    print(f"OOV Terms: {len(oov)}")    N = len(dataset)    n = sum(candidates.values())    print(f'Coverage: {n / N * 100:2.1f} {n}/{N}')    return oovdef score_source_vocabs(entities, umls, concepts, ignore={}):    """    Given a collection of entity annotations, score coverage by    STY/SAB pair in the UMLS    """    # normalize names    concepts = {name.replace(' ', '_').lower() for name in concepts}    scores = collections.Counter()    for c in entities:        # common term variations        term = re.sub(r'''\s''', ' ', c.text)        term_cands = set([            term,            term.lower(),            term.rstrip('s').lower()  # remove plural        ])        for sab, sty in umls.dictionary:            # ignore certain sem types            if sty not in concepts or sty in ignore:                continue            d = umls.dictionary[(sab, sty)]            matched = False            for t in term_cands:                if t in d:                    matched = True                    break            if matched:                scores[(sab, sty)] += 1    return sorted(scores.items(), key=lambda x: x[-1], reverse=1)def umls_classmap_dicts(sabs, class_map, umls):    """    Merge UMLS ontologies into class-specific dictionaries    """    summary = {y:0 for y in np.unique(list(class_map.values()))}    merged_dicts = collections.defaultdict(dict)    for sab,sty in umls.dictionary:        # skip STYs not in the class map        if sty not in class_map:            continue        y = class_map[sty]        d = dict.fromkeys(umls.dictionary[(sab, sty)])        summary[y] += 1        if sab in sabs:            merged_dicts[f'{sab}_{y}'].update(d)        else:            merged_dicts[f'OTHER_{y}'].update(d)    print('Merged STYs:', summary)    print('Merged SABs:', {y:len([name for name in merged_dicts if re.search(f'_{y}', name)]) for y in summary} )    return merged_dictsdef umls_ontology_dicts(sabs, class_map, umls):    """    Merge UMLS ontologies into class-specific dictionaries    SAB -> TERMS -> probas    """    # group by SAB    classes = set(class_map.values())    ontologies = collections.defaultdict(dict)    for sab, sty in umls.dictionary:        ontologies[sab][sty] = umls.dictionary[(sab, sty)]    for sab in ontologies:        probas = collections.defaultdict(list)        for sty in ontologies[sab]:            if sty not in class_map:                continue            for term in ontologies[sab][sty]:                label = class_map[sty]                probas[term].append(label)        for term in probas:            p = []            for y in sorted(classes):                p.append(probas[term].count(y))            probas[term] = np.array(p).astype(np.float32)        ontologies[sab] = probas    # pool remaining ontologies into OTHER category    other = {}    f_ontologies = {}    for sab in ontologies:        if sab in sabs:            f_ontologies[sab] = ontologies[sab]            continue        for term in ontologies[sab]:            if term not in other:                other[term] = ontologies[sab][term]            else:                other[term] += ontologies[sab][term]    f_ontologies['OTHER'] = other    ontologies = f_ontologies    # compute probabilities    for sab in ontologies:        for term in ontologies[sab]:            ontologies[sab][term] = \                ontologies[sab][term] / np.sum(ontologies[sab][term])    return ontologies